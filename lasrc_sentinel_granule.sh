#!/bin/bash

# Exit on any error
set -o errexit

id=$SENTINEL_SCENE_ID
workingdir="/tmp/${id}"
safedirectory="/tmp/${id}/${id}.SAFE"
safezip="/tmp/${id}/${id}.zip"
fmask="/tmp/${id}/fmask.img"
fmaskbin="${workingdir}/fmask.bin"

# Remove tmp files on exit
trap "rm -rf $workingdir; exit" INT TERM EXIT

mkdir "$workingdir"

bucket=$OUTPUT_BUCKET
IFS='_'
# Read into an array as tokens separated by IFS
read -ra ADDR <<< "$id"

# Format GCS url and download
url=gs://gcp-public-data-sentinel-2/tiles/${ADDR[5]:1:2}/${ADDR[5]:3:1}/${ADDR[5]:4:2}/${id}.SAFE
gsutil -m cp -r "$url" "$workingdir"

# Run Fmask
# fmask_sentinel2Stacked.py -o "$fmask" --safedir "$safedirectory"

# Convert to flat binary
# gdal_translate -of ENVI "$fmask" "$fmaskbin"

# Zips and unpacks S2 SAFE directory.  The ESA SAFE data will be provided zipped.
cd "$workingdir"
zip -r "$safezip" "${id}.SAFE"
rm -rf "${id}.SAFE"
unpackage_s2.py -i "$safezip" -o "$workingdir"
rm "$safezip"

# granuledirectory="${safedirectory}/GRANULE"
# Move metadata files to IMG_DATA for ESPA conversion
# cd "$granuledirectory"

# granule_id=$(find . -maxdepth 1 -type d -name '[^.]?*' -printf %f -quit)
# workingdir=/tmp/${id}.SAFE/GRANULE/${granule_id}/IMG_DATA/

# cp "/tmp/${id}.SAFE/MTD_MSIL1C.xml" "$workingdir"
# cp "/tmp/${id}.SAFE/GRANULE/${granule_id}/MTD_TL.xml" "$workingdir"

# cd "$workingdir"


# espa_xml="${id}.xml"
# outputhdf="${id}_out.hdf"

# Convert to espa format
cd "$safedirectory"
convert_sentinel_to_espa

# Get the new granule id generated by the ESPA conversion
espa_xml=$(find . -type f -name "*.xml" ! -name "MTD_TL.xml" ! -name "MTD_MSIL1C.xml" -exec basename \{} \;)
espa_id="${espa_xml%.*}"

# Run lasrc
do_lasrc_sentinel.py --xml "$espa_xml"

hls_espa_one_xml="${espa_id}_1_hls.xml"
hls_espa_two_xml="${espa_id}_2_hls.xml"
sr_hdf_one="${espa_id}_sr_1.hdf"
sr_hdf_two="${espa_id}_sr_2.hdf"
hls_sr_combined_hdf="${espa_id}_sr_combined.hdf"

# Create ESPA xml files using HLS v1.5 band names.
create_sr_hdf_file.py -i "$espa_xml" -o "$hls_espa_one_xml" -f one
create_sr_hdf_file.py -i "$espa_xml" -o "$hls_espa_two_xml" -f two

# Convert ESPA xml files to HDF
convert_espa_to_hdf --xml="$hls_espa_one_xml" --hdf="$sr_hdf_one"
convert_espa_to_hdf --xml="$hls_espa_two_xml" --hdf="$sr_hdf_two"

# Combine split hdf files and resample 10M SR bands back to 20M and 60M.
twohdf2one "$sr_hdf_one" "$sr_hdf_two" MTD_MSIL1C.xml MTD_TL.xml LaSRC "$hls_sr_combined_hdf"

# Run addFmaskSDS
# addFmaskSDS "$srhdf" "$fmaskbin" "$mtl" "LaSRC" "$outputhdf" >&2
# if [ $? -ne 0 ]
# then
	# echo "Error in addFmaskSDS: $outputhdf" >&2
	# echo "Line $LINENO of ${BASH_SOURCE[0]}" >&2
	# exit 1
# fi

cd "$workingdir"
# Copy files to S3
aws s3 sync . "s3://${bucket}/${id}/"
